name: build-and-deploy

on:
  push:
    branches: [ main ]

permissions:
  contents: read
  id-token: write
  packages: write

env:
  APP_NAME: app-svc
  NAMESPACE: dev

jobs:
  build:
    runs-on: ubuntu-latest
    outputs:
      image: ${{ steps.meta.outputs.image }}
      image_tag: ${{ steps.meta.outputs.image_tag }}
    steps:
      - uses: actions/checkout@v4

      - uses: docker/setup-buildx-action@v3

      - uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build & push image
        run: |
          IMAGE="ghcr.io/${{ github.repository }}/${{ env.APP_NAME }}:${{ github.sha }}"
          docker build -t "$IMAGE" .
          docker push "$IMAGE"
          echo "IMAGE=$IMAGE" >> $GITHUB_ENV
          echo "IMAGE_TAG=${{ github.sha }}" >> $GITHUB_ENV

      - name: Set outputs
        id: meta
        run: |
          echo "image=${IMAGE}" >> $GITHUB_OUTPUT
          echo "image_tag=${IMAGE_TAG}" >> $GITHUB_OUTPUT

  deploy-dev:
    runs-on: ubuntu-latest
    needs: build
    environment: dev
    permissions:
      id-token: write
      contents: read
    steps:
      - uses: actions/checkout@v4

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: latest

      - name: Install Helm
        uses: azure/setup-helm@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}
          role-skip-session-tagging: true

      # Quick sanity: can we see the cluster and its endpoint?
      - name: Describe cluster
        run: |
          aws sts get-caller-identity
          aws eks describe-cluster --name "${{ secrets.EKS_CLUSTER }}" --region "${{ secrets.AWS_REGION }}" --query "cluster.{name:name,endpoint:endpoint,publicAccess:resourcesVpcConfig.endpointPublicAccess}" --output table

      # Force-generate kubeconfig with exec auth
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name "${{ secrets.EKS_CLUSTER }}" --region "${{ secrets.AWS_REGION }}" --alias fincore
          echo "==== kubeconfig contexts ===="
          kubectl config get-contexts
          kubectl config use-context fincore
          echo "==== try listing namespaces ===="
          kubectl get ns

      # If API server requires private access, bail out early with a clear message
      - name: Check API reachability
        run: |
          if ! kubectl version --short; then
            echo "::error::Unable to reach EKS API server from GitHub runner.
            If your EKS endpoint is PRIVATE, use a self-hosted runner in the VPC,
            temporarily enable public endpoint, or tunnel via a jump host." && exit 1
          fi

      - name: Helm upgrade
        run: |
          helm upgrade --install "${{ env.APP_NAME }}" ./helm \
            --namespace "${{ env.NAMESPACE }}" --create-namespace \
            --set image.repository="ghcr.io/${{ github.repository }}" \
            --set image.tag="${{ needs.build.outputs.image_tag }}"