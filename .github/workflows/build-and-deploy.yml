name: build-and-deploy

on:
  push:
    branches: [ main ]

permissions:
  contents: read
  id-token: write
  packages: write

env:
  APP_NAME: app-svc
  NAMESPACE: dev

jobs:
  build:
    runs-on: ubuntu-latest
    outputs:
      image: ${{ steps.meta.outputs.image }}
      image_tag: ${{ steps.meta.outputs.image_tag }}
    steps:
      - uses: actions/checkout@v4

      - uses: docker/setup-buildx-action@v3

      - uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build & push image
        run: |
          IMAGE="ghcr.io/${{ github.repository }}:${{ github.sha }}"  # -> ghcr.io/ksunique/app-svc:<sha>
          # IMAGE="ghcr.io/${{ github.repository }}/${{ env.APP_NAME }}:${{ github.sha }}"
          docker build -t "$IMAGE" .
          docker push "$IMAGE"
          echo "IMAGE=$IMAGE" >> $GITHUB_ENV
          echo "IMAGE_TAG=${{ github.sha }}" >> $GITHUB_ENV

      - name: Set outputs
        id: meta
        run: |
          echo "image=${IMAGE}" >> $GITHUB_OUTPUT
          echo "image_tag=${IMAGE_TAG}" >> $GITHUB_OUTPUT

  deploy-dev:
    runs-on: ubuntu-latest
    needs: build
    environment: dev
    permissions:
      id-token: write
      contents: read
    steps:
      - uses: actions/checkout@v4

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          # Pin to a recent version; adjust to your cluster minor if you prefer
          version: 'v1.29.0'

      - name: Install Helm
        uses: azure/setup-helm@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}
          role-skip-session-tagging: true

      - name: Describe cluster
        run: |
          aws sts get-caller-identity
          aws eks describe-cluster --name "${{ secrets.EKS_CLUSTER }}" --region "${{ secrets.AWS_REGION }}" \
            --query "cluster.{name:name,endpoint:endpoint,publicAccess:resourcesVpcConfig.endpointPublicAccess}" --output table

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name "${{ secrets.EKS_CLUSTER }}" --region "${{ secrets.AWS_REGION }}" --alias fincore
          kubectl config use-context fincore
          kubectl config get-contexts

      # âœ… Cross-version API reachability check (no --short)
      - name: Check API reachability
        run: |
          set -e
          # Try unauthenticated-ish endpoints first; fall back to a cheap auth call
          if ! kubectl get --raw='/version' >/dev/null 2>&1 && \
             ! kubectl get --raw='/healthz'  >/dev/null 2>&1 && \
             ! kubectl get ns --request-timeout=5s >/dev/null 2>&1 ; then
            echo "::error::Unable to reach EKS API server from GitHub runner.
            If your EKS endpoint is PRIVATE, use a self-hosted runner in the VPC,
            temporarily enable public endpoint, or tunnel via a jump host."
            exit 1
          fi
          echo "EKS API reachable."

      - name: Helm upgrade
        run: |
          helm upgrade --install "${{ env.APP_NAME }}" ./helm \
            --namespace "${{ env.NAMESPACE }}" --create-namespace \
            --set image.repository="ghcr.io/${{ github.repository }}" \
            --set image.tag="${{ needs.build.outputs.image_tag }}"      